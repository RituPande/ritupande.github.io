---
layout: post
title: Noise Contrastive Estimation and Negative Sampling
published: true  
---

***Noise Contrastive Estimaion (NCE)*** is technique used to approximate expectation of a function. It is an evolution of *importance sampling* technique. Importance sampling is an approximation technique which draws samples from a distribution different from the distribution required to be approximated and applies a correction to account for the it [1]. This technique is used when sampling from original distribution is difficult, or if a better approximation can be achieved using *importance sampling* than via  other techniques [2]. NCE, on the other hand, uses a disribution with one positive and k negative samples ( or noise ) from the empirical distibution to approximate it.   

## 1. Empirical Distribution, Noise Distribution and Model Distribution
Before delving further into details for NCE, it is important that we define the following terms and understand the notation used in this paper:  
  
**Empirical Distribution:** Empirical distributions, referred in the paper as **$$ \hat{p} $$**, is distribution of observed data of a p.d.f. Unlike the pdf of the distribution which is observed , the probability of each observation in a empirical distribution is $$  \frac{1}{m} $$, for  i.i.d. distribution where m is the number of observed samples. Therefore,  

$$ \displaystyle E_{\hat{p} }(X) = \frac{1}{m} \sum_1^n X_i $$  
  
**Noise Distribution:**  For a binary classification problem, a noise distribution contains samples from the empirical distribution that correspond to 0 ground-truth labels. In this paper a noise disribution is referred to as **$$ q $$**.   

**Model Distribution:** A model find a parameter $$ \theta $$  that approximates the empirical distribution as closely as possible. The data generated by such a model is called model distribution. Model distribution is referred to as $$ p_{\theta} $$ in this paper.  

## 2 Noise Contrastive Estimation 

Noise contrastive estimation is used in WordToVEC and NodeToVEC algorithms which learn low dimensional vector representation of words and nodes respectively, The vectors of the words or nodes that are related are more similar than  vectors of words or nodes that are unrelated using unlabeled data. Here is a brief overview of how these algorithms convert this unsupervised learning problem to a supervised one and what role NCE plays in it: 

1.  The problem is initially formulated as a  multi-class classification problem which predicts whether an entity is suitable to appear in context of another in an empirical distribution. In WordToVEC algorithm it ditermines whether a word appears in context of another word and in NodeToVEC to ditermine if a node is in neighborhood of another in the training set.   
    
2. As a first step, another dataset is created from the orignal one. which contains multiple pairs of a context entity c and another entity which is predicted to appear in the context of c. A training sample $$ ( c_i, entity_{ij} ) $$ generated in such a manner is labeled 1 if $$ entity_{ij} $$ appears in context of $$ c_i $$ and 0 otherwise.  This now becomes a supervised  multi-class classification problem, wherin the output layer of the model predicts the probbaility of each entity in the vocabulary being in context of c. The output layer therefore uses a softmax function:  
$$  \displaystyle p_{\theta}( e|c ) = \frac{s_{\theta}(e,c)}{\sum_{e' \in V} s_{\theta}(e',c)}  $$  
&emsp; &ensp; where V is the set of all possible entities called 'vocabulary'.  
  
3. The problem in the above equation is that if  \|V\| is very large, each training iteration would require a large number of weights to be updated, making the problem computationally too expensive. This is where NCE comes into picture.  
    
4. NCE transforms this multi-class classification problem to a binary classification problem and hence drastically reduces the number of weights that are required to be updated for each iteration.   
  
### 2.1 Mathematical foundation

We can define the loss function of a binary classiciation problem as follows:  

$$  L_{nce_k} = $$ Log likihood of positive training samples + Log Likelihood of negative training samples 

For NCE, for any given context $$ c_i $$, we take one postive training sample s.t. $$ (c_i, e_i ) $$ is labeled 1  and  k negative samples  s.t. $$ (c_i, e_j ), s.t. j \in {1,2,...,K} , e_j $$ is sampled from q  is labeled 0 .   
  
Let $$ L_{neg} $$ be the log likelihood of the negative training samples:  

$$ \displaystyle L_{neg}  = \sum_{j=1}^k log[]p( D = 0 | e_j,c_i ) $$  

Multiplying and dividing by k:  

$$ \displaystyle \displaystyle L_{neg}  = k * \frac{1}{k} \sum_{j=1}^k log[]p( D = 0 | e_j,c_i ) $$  
    
Bringing $$ \frac{1}{k} $$ inside summation:  
  
$$ \displaystyle L_{neg}  = k *  \sum_{j=1}^k  \frac{1}{k} log[]p( D = 0 | e_j,c_i ) $$  
   
$$ \frac{1}{k} $$ is the probability of each training sample , from a total of k, sampled from noise distribution q.  
   
$$ \displaystyle L_{neg}  = k *  \sum_{j=1}^k  q(e_j) log[]p( D = 0 | e_j,c_i ) $$  
  
By definition of expectation:  
  
$$ \displaystyle L_{neg} = k * E_{e_j \~ q} log[]p( D = 0 | e_j,c_i ) $$  
   
Summing the log likelihoods of positive and negative training samples
$$ L_{nce_k} =  log[]p( D = 1 | e_i,c_i )  +  k * E_{e_j ~q} log[]p( D = 0 | e_j,c_i ) $$
  
$$  \displaystyle E_{e_j ~q} log[]p( D = 0 | e_j,c_i ) $$  is approximated using Monte-Carlo approximation: 

$$  \displaystyle L_{nce_k}^MC =  log[]p( D = 1 | e_i,c_i )  +  k *  \frac{1}{k} \sum_{j=1, e_j ~ q }^k  log[]p( D = 0 | e_j,c_i ) $$    

**$$ L_{nce_k}^MC =  log[]p( D = 1 | e_i,c_i )  + \sum_{j=1, e_j ~ q }^k  log[]p( D = 0 | e_j,c_i ) $$**  

Now, what remains is knowing how to find  $$ p( D = 1 | e_i,c_i ) and p( D = 0 | e_j,c_i ) $$

Owing to how the positive and negative samples are created, we have:

$$ p(d ,e' | c) = \frac{k}{k+1} * q (e') $$, for d = 0  ...    eq 1.
$$ p(d ,e | c) = \frac{1}{k+1} * p\hat (e|c ) $$  for d = 1 ... eq 2.  

By definition of conditional probability:  
  
$$  \displaystyle p(d ,e | c) = \frac { p(d, e, c)}{ p(c)} $$  
  
Mulitiplying and dividing by p(e,c)  
  
$$  \displaystyle p(d ,e | c) = \frac { p(d, e, c) * p(e,c)}{ p(c) * p(e,c)} $$  
   
$$  \displaystyle p(d ,e | c) = \frac{\frac { p(d, e, c)}{p(e,c)} }{\frac{ p(c)}{p(e,c)} } $$  
  
$$  \displaystyle p(d ,e | c) =  p(d| e, c) * p(e |c)  $$  
  
$$  \displaystyle p(d | e, c) =  \frac{p(d, e | c)}{ p(e |c) }  $$  
  
  
The denominator on RHS is the probability of the numerator marginalized over d.    
  
Replacing eq 1 and eq 2 in the formula of p(d | e, c)  and replacing  $$ p\hat (e|c ) $$ with s_{\theta}(e,c) $$  where $$ s_{\theta}(e,c) $$ is a score function obtained from the model, we have:    
    
$$ \displaystyle p(d  = 0| e', c) = \frac{k* q(e)}{s_{\theta}(e,c) + k * q(e) } $$  
  
$$ \displaystyle p(d  = 1| e, c) = \frac{s_{\theta}(e,c)}{s_{\theta}(e,c) + k * q(e) } $$  

These values can be plugged in the equation for $$ L_{nce_k}^MC $$ to get the value of the loss function.  

## 3. Negative Sampling.

Negative sampling is a variation of NCE which defines conditional probabilities as:

$$ \displaystyle p(d  = 0| e', c) = \frac{1}{s_{\theta}(e,c) + 1} $$    
  
$$ \displaystyle p(d  = 1| e, c) = \frac{s_{\theta}(e,c)}{s_{\theta}(e,c) +1 } $$  

This objective can be viewed as  NCE where  k = \|V\|  and q is uniform ( i.e. with probability \frac{1}{\|V\|} } making k* q(e) =  1. Hence, for values of $$ k < \|V\| $$, negative sampling might be suitable but does not provide asymptotic consistency guarantees that NCE has.  


## References  
[1]. [ML 17.5 Importance sampling -Introduction](https://www.youtube.com/watch?v=S3LAOZxGcnk)    
[2]. [ML 17.6 Importance sampling- Intuition](https://www.youtube.com/watch?v=3Mw6ivkDVZc)  
