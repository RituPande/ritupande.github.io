---
layout: post
title: Noise Contrastive Estimaion, Negative Sampling and its Application to Graph Learning
published: false  
---

***Noise Contrastive Estimaion (NCE)*** is technique used to approximate expectation of a function. It is an evolution of *importance sampling* technique. Importance sampling is an approximation technique which draws samples from a distribution different from the distribution required to be approximated and applies a correction to account for the it [1]. This technique is used when sampling from original distribution is difficult, or if a better approximation can be achieved using *importance sampling* than via  other techniques [2]. NCE, on the other hand, uses a disribution with one positive and k negative samples ( or noise ) from the empirical distibution to approximate it.   

## 1. Empirical Distribution, Noise Distribution and Model Distribution
Before delving further into details for NCE, it is important that we define the following terms and understand the notation used in this paper:  
  
**Empirical Distribution:** Empirical distributions, referred in the paper as **$$ p\hat $$**, is distribution of observed data of a p.d.f. Unlike the pdf of the distribution which is observed , the probability of each observation in a empirical distribution is $$ \displaystyle \frac{1}{m} $$, for  i.i.d. distribution where m is the number of observed samples. Therefore,  

$$\displaystyle E_{p\hat}(X) = \frac{1}{m} \sum_1^n X_i $$  
  
**Noise Distribution:**  For a binary classification problem, a noise distribution contains samples from the empirical distribution that correspond to 0 ground-truth labels. In this paper a noise disribution is referred to as **$$ q\hat $$**.   

**Model Distribution:** A model find a parameter $$ \theta $$  that approximates the empirical distribution as closely as possible. The data generated by such a model is called model distribution. Model distribution is referred to as $$ p\theta $$ in this paper.  

## Noise Contrastive Estimation 

Noise contrastive estimation is used in WordToVEC and NodeToVEC algorithms to learn low dimensional vector representation of words and nodes respectively, s.t. the vectors of the words or nodes that are related are more similar than  vectors of words or nodes that are unrelated.  This is achieved by formulating the problem as a binary classification problem used to predict whether an entity is suitable to appear in context of another in an empirical distribution. It is used in WordToVEC algorithm to ditermine whether a word appears in context of another word and in NodeToVEC to ditermine if a node is in neighborhood of another in the training set.  
  
To formulate such a problem as a supervised learning problem from unlabled training dataset, as a first step, we create another dataset which contains a pair of context entity c and one other entity we are trying to predict as to whether it can  appear in it's context. A training sample $$ ( c, entity_i ) $$ generated in such a manner is labeled 1 if entity_1 is relevant in context of c and 0 otherwise.




## References  
[1]. [ML 17.5 Importance sampling -Introduction](https://www.youtube.com/watch?v=S3LAOZxGcnk)    
[2]. [ML 17.6 Importance sampling- Intuition](https://www.youtube.com/watch?v=3Mw6ivkDVZc)  
