---
layout: post
title: Noise Contrastive Estimaion, Negative Sampling and its Application to Graph Learning
published: false  
---

***Noise Contrastive Estimaion (NCE)*** is technique used to approximate expectation of a function. It is an evolution of *importance sampling* technique. Importance sampling is an approximation technique which draws samples from a distribution different from the distribution required to be approximated and applies a correction to account for the it [1]. This technique is used when sampling from original distribution is difficult, or if a better approximation can be achieved using *importance sampling* than via  other techniques [2]. NCE, on the other hand, uses a disribution with one positive and k negative samples ( or noise ) from the empirical distibution to approximate it.   

## 1. Empirical Distribution, Noise Distribution and Model Distribution
Before delving further into details for NCE, it is important that we define the following terms and understand the notation used in this paper:  
  
**Empirical Distribution:** Empirical distributions, referred in the paper as **$$ p\hat $$**, is distribution of observed data of a p.d.f. Unlike the pdf of the distribution which is observed , the probability of each observation in a empirical distribution is $$ \displaystyle \frac{1}{m} $$, for  i.i.d. distribution where m is the number of observed samples. Therefore,  

$$\displaystyle E_{p\hat}(X) = \frac{1}{m} \sum_1^n X_i $$  
  
**Noise Distribution:**  For a binary classification problem, a noise distribution contains samples from the empirical distribution that correspond to 0 ground-truth labels. In this paper a noise disribution is referred to as **$$ q $$**.   

**Model Distribution:** A model find a parameter $$ \theta $$  that approximates the empirical distribution as closely as possible. The data generated by such a model is called model distribution. Model distribution is referred to as $$ p\theta $$ in this paper.  

## Noise Contrastive Estimation 

Noise contrastive estimation is used in WordToVEC and NodeToVEC algorithms which learn low dimensional vector representation of words and nodes respectively, The vectors of the words or nodes that are related are more similar than  vectors of words or nodes that are unrelated using unlabeled data. Here is a brief overview of how these algorithms convert this unsupervised learning problem to a supervised one and what role NCE plays in it: 

1.  The problem is initially formulated as a  multi-class classification problem which predicts whether an entity is suitable to appear in context of another in an empirical distribution. In WordToVEC algorithm it ditermines whether a word appears in context of another word and in NodeToVEC to ditermine if a node is in neighborhood of another in the training set.   
    
2. As a first step, another dataset is created from the orignal one. which contains multiple pairs of a context entity c and another entity which is predicted to appear in the context of c. A training sample $$ ( c_i, entity_ij ) $$ generated in such a manner is labeled 1 if $$ entity_ij $$ appears in context of c_i and 0 otherwise.  This now becomes a supervised  multi-class classification problem, wherin the output layer of the model predicts the probbaility of each entity in the vocabulary being in context of c. The output layer therefore uses a softmax function:  
$$ p_{\theta}( e|c ) = \frac{s_{\theta}(e,c)}{\sum_{e' \in V} s{\theta}(e',c)}  $$  

  &emsp; &ensp; where V is the set of all possible entities called 'vocabulary'.  
  
3. The problem in the above equation is that if  |V| is very large each training iteration .would require a large number of weights to be updated, making the problem computationally too expensive. This is where NCE comes into picture.  
    
4. NCE transforms this multi-class classification problem to a binary classification problem and hence drastically reduces the number of weights that are required to be updated for each iteration. 

### Mathematical foundation

We can define the loss function of a binary classiciation problem as follows:

$$ L_{nce_k} = Log likihood of positive training samples + Log Likelihood of negative training samples 

For NCE, for any given context $$ c_i $$, we take one postive training sample s.t. $$ (c_i, e_i ) $$ is labeled 1  and  k negative samples  s.t. $$ (c_i, e_j ), s.t. j \n {1,2,...,K} and e_j is sampled from q $$ is labeled 0, .   
  
Let $$ L_{neg} $$ be the log likelihood of the negative training samples:  

$$ \displaystle L_{neg}  = \sum_{j=1}^k log p( D = 0 | e_j,c_i ) $$  

Multiplying and dividing by k:  

$$ \displaystyle L_{neg}  = k * \frac{1}{k} \sum_{j=1}^k log p( D = 0 | e_j,c_i ) $$  
    
Bringing $$ \frac{1}{k} $$ inside summation:  
  
$$ \displaystyle L_{neg}  = k *  \sum_{j=1}^k  \frac{1}{k} log p( D = 0 | e_j,c_i ) $$  
   
$$ \frac{1}{k} $$ is the probability of each training sample , from a total of k, sampled from noise distribution q.  
   
$$ \displaystyle L_{neg}  = k *  \sum_{j=1}^k  q(e_j) log p( D = 0 | e_j,c_i ) $$  
  
By definition of expectation:
  
$$ \displaystyle L_{neg} = k * E_{e_j ~q} log p( D = 0 | e_j,c_i ) $$  
   
Summing the log likelihoods of positive and negative training samples
$$ L_{nce_k} =  log p( D = 1 | e_i,c_i )  +  k * E_{e_j ~q} log p( D = 0 | e_j,c_i ) $$
  
E_{e_j ~q} log p( D = 0 | e_j,c_i ) is approximated using Monte-Carlo approximation: 

$$ L_{nce_k}^MC =  log p( D = 1 | e_i,c_i )  +  k *  \frac{1}{k} \sum_{j=1, e_j ~ q }^k  log p( D = 0 | e_j,c_i ) $$    

$$ L_{nce_k}^MC =  log p( D = 1 | e_i,c_i )  + \sum_{j=1, e_j ~ q }^k  log p( D = 0 | e_j,c_i ) $$    



## References  
[1]. [ML 17.5 Importance sampling -Introduction](https://www.youtube.com/watch?v=S3LAOZxGcnk)    
[2]. [ML 17.6 Importance sampling- Intuition](https://www.youtube.com/watch?v=3Mw6ivkDVZc)  
