---
layout: post
title: Noise Contrastive Estimaion, Negative Sampling and its Application to Graph Learning
published: false  
---

***Noise Contrastive Estimaion (NCE)*** is technique used to approximate expectation of a function. It is an evolution of *importance sampling* technique. Importance sampling is an approximation technique which draws samples from a distribution different from the distribution required to be approximated and applies a correction to account for the it [1]. This technique is used when sampling from original distribution is difficult, or if a better approximation can be achieved using *importance sampling* than via  other techniques [2]. NCE, on the other hand, uses a disribution with one positive and k negative samples ( or noise ) from the original distibution to approximate it.   

## 1. Empirical Distribution, Model Distibution and Noise Disributions
Before delving further into details for NCE, it is important that we define the following terms and understand the notation used in this paper:  
  
**Empirical Distribution:** Empirical distributions, referred in the paper as $$ p\hat $$, is distribution of observed data of a p.d.f. Unlike the pdf of the distribution which is observed , the probability of each observation in a emperical distribution is $$ \displaystyle \frac{1}{m} $$, where m is the number of observed samples. Therefore,  

$$\displaystyle E_{p\hat}(X) = \frac{1}{m} \sum_1^n X_i $$  
  
**Noise Distribution:**  


## References  
[1]. ML 17.5 Importance sampling -Introduction  
[2]. ML 17.6 Importance sampling- Intuition  
